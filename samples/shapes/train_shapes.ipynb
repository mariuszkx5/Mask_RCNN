{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "train_shapes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariuszkx5/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsRN7cqhBpsQ",
        "colab_type": "text"
      },
      "source": [
        "# Mask R-CNN - Train on Shapes Dataset\n",
        "\n",
        "\n",
        "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
        "\n",
        "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXnYTZ0e3E76",
        "colab_type": "text"
      },
      "source": [
        "samples/shapes/train_shapes.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0kGpa_OJquy",
        "colab_type": "code",
        "outputId": "ef10da86-b5fb-4383-d1b5-e196a38a5906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "pip install tensorflow==1.14"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.29.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (47.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsUXRLNqLJEE",
        "colab_type": "code",
        "outputId": "4df9ea10-ce91-439c-8d00-208dafe00fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "pip install keras==2.1.0"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.0) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.0) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.0) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjd-qG4MvlQh",
        "colab_type": "code",
        "outputId": "328b894a-36d5-4cc6-dcb2-b8799bb552b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgyjqdF1w4pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO_2126QwgE8",
        "colab_type": "code",
        "outputId": "bd0dc340-8339-4efa-a4ca-fcfab52ee863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "sys.path "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " '/content/drive/My Drive/Mask_RCNN',\n",
              " '/',\n",
              " '/content/drive/My Drive/Mask_RCNN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqrm-6PkBpsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory of the project\n",
        "ROOT_DIR = '/content/drive/My Drive/Mask_RCNN'\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJWDgKKdCN5j",
        "colab_type": "code",
        "outputId": "3638d859-a786-4ad6-a5e6-59e6c6f1874c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vtLujuOJAOe",
        "colab_type": "code",
        "outputId": "b1acb63f-66ac-4490-9b76-7dc469c1d73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWH8XwIQIh04",
        "colab_type": "code",
        "outputId": "3b237c62-cc55-4dd5-f10b-7186e14fb10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 5144832378157903624, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13569791418654744153\n",
              " physical_device_desc: \"device: XLA_CPU device\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF5YcAoUBpsb",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuQ3tmx0Bpse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "outputId": "c732b099-7e69-4e51-ff65-07c51db67f03"
      },
      "source": [
        "class ShapesConfig(Config):\n",
        "    \"\"\"Configuration for training on the toy shapes dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the toy shapes dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 8\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "\n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
        "\n",
        "    # Reduce training ROIs per image because the images are small and have\n",
        "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "\n",
        "    # Use a small epoch since the data is simple\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # use small validation steps since the epoch is small\n",
        "    VALIDATION_STEPS = 5\n",
        "    \n",
        "config = ShapesConfig()\n",
        "config.display()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     8\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 8\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  128\n",
            "IMAGE_META_SIZE                16\n",
            "IMAGE_MIN_DIM                  128\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [128 128   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           shapes\n",
            "NUM_CLASSES                    4\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                100\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           32\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               5\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idh3V5tvBpsj",
        "colab_type": "text"
      },
      "source": [
        "## Notebook Preferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvTdr_5SBpsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaVZe1YKBpsq",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Create a synthetic dataset\n",
        "\n",
        "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
        "\n",
        "* load_image()\n",
        "* load_mask()\n",
        "* image_reference()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIN4H-V-Bpsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShapesDataset(utils.Dataset):\n",
        "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
        "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
        "    The images are generated on the fly. No file access required.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_shapes(self, count, height, width):\n",
        "        \"\"\"Generate the requested number of synthetic images.\n",
        "        count: number of images to generate.\n",
        "        height, width: the size of the generated images.\n",
        "        \"\"\"\n",
        "        # Add classes\n",
        "        self.add_class(\"shapes\", 1, \"square\")\n",
        "        self.add_class(\"shapes\", 2, \"circle\")\n",
        "        self.add_class(\"shapes\", 3, \"triangle\")\n",
        "\n",
        "        # Add images\n",
        "        # Generate random specifications of images (i.e. color and\n",
        "        # list of shapes sizes and locations). This is more compact than\n",
        "        # actual images. Images are generated on the fly in load_image().\n",
        "        for i in range(count):\n",
        "            bg_color, shapes = self.random_image(height, width)\n",
        "            self.add_image(\"shapes\", image_id=i, path=None,\n",
        "                           width=width, height=height,\n",
        "                           bg_color=bg_color, shapes=shapes)\n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly from the\n",
        "        specs in image_info.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
        "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
        "        image = image * bg_color.astype(np.uint8)\n",
        "        for shape, color, dims in info['shapes']:\n",
        "            image = self.draw_shape(image, shape, dims, color)\n",
        "        return image\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the shapes data of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"shapes\":\n",
        "            return info[\"shapes\"]\n",
        "        else:\n",
        "            super(self.__class__).image_reference(self, image_id)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        shapes = info['shapes']\n",
        "        count = len(shapes)\n",
        "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
        "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
        "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
        "                                                shape, dims, 1)\n",
        "        # Handle occlusions\n",
        "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
        "        for i in range(count-2, -1, -1):\n",
        "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
        "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
        "        # Map class names to class IDs.\n",
        "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
        "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
        "\n",
        "    def draw_shape(self, image, shape, dims, color):\n",
        "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
        "        # Get the center x, y and the size s\n",
        "        x, y, s = dims\n",
        "        if shape == 'square':\n",
        "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
        "        elif shape == \"circle\":\n",
        "            cv2.circle(image, (x, y), s, color, -1)\n",
        "        elif shape == \"triangle\":\n",
        "            points = np.array([[(x, y-s),\n",
        "                                (x-s/math.sin(math.radians(60)), y+s),\n",
        "                                (x+s/math.sin(math.radians(60)), y+s),\n",
        "                                ]], dtype=np.int32)\n",
        "            cv2.fillPoly(image, points, color)\n",
        "        return image\n",
        "\n",
        "    def random_shape(self, height, width):\n",
        "        \"\"\"Generates specifications of a random shape that lies within\n",
        "        the given height and width boundaries.\n",
        "        Returns a tuple of three valus:\n",
        "        * The shape name (square, circle, ...)\n",
        "        * Shape color: a tuple of 3 values, RGB.\n",
        "        * Shape dimensions: A tuple of values that define the shape size\n",
        "                            and location. Differs per shape type.\n",
        "        \"\"\"\n",
        "        # Shape\n",
        "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
        "        # Color\n",
        "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
        "        # Center x, y\n",
        "        buffer = 20\n",
        "        y = random.randint(buffer, height - buffer - 1)\n",
        "        x = random.randint(buffer, width - buffer - 1)\n",
        "        # Size\n",
        "        s = random.randint(buffer, height//4)\n",
        "        return shape, color, (x, y, s)\n",
        "\n",
        "    def random_image(self, height, width):\n",
        "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
        "        Returns the background color of the image and a list of shape\n",
        "        specifications that can be used to draw the image.\n",
        "        \"\"\"\n",
        "        # Pick random background color\n",
        "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
        "        # Generate a few random shapes and record their\n",
        "        # bounding boxes\n",
        "        shapes = []\n",
        "        boxes = []\n",
        "        N = random.randint(1, 4)\n",
        "        for _ in range(N):\n",
        "            shape, color, dims = self.random_shape(height, width)\n",
        "            shapes.append((shape, color, dims))\n",
        "            x, y, s = dims\n",
        "            boxes.append([y-s, x-s, y+s, x+s])\n",
        "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
        "        # shapes covering each other\n",
        "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
        "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
        "        return bg_color, shapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RaQ5TR3Bpsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training dataset\n",
        "dataset_train = ShapesDataset()\n",
        "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = ShapesDataset()\n",
        "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSpb-GphBps2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "ac7c940f-d996-408f-eb5c-bf8a9e194b01"
      },
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdAElEQVR4nO3deXxU5dn/8e81k4WQhLAvCsomiKioKOCGCK60LnWp+tStasX+bCuP1FZt/bVV6+5T66N1Qa3WfVesUjdwAUEQFWVxF9kJYQvZMzP380cmGJMJzCRz5kwyn/frxYuZM2fu+4qeMOc711nMOScAAAAAiEfA7wIAAAAAtB0ECAAAAABxI0AAAAAAiBsBAgAAAEDcCBAAAAAA4kaAAAAAABA33wOEmfU3szcaLfuqBeO8Ymb7Rh9PNLNNZmbR5zeZ2VlxjHGNmX3XsB4z29fMZpvZO2Y2w8wGRpcPjC57y8xmmlnf7Yw7yMwWmFmZmR3SYPltZjY3+ufyBsuvMLP5ZjbPzC5N9L8F/GVmnc3s7GZeu83MeiRpnia/O0CizKy3md2awPoJ//sMAGhffA8QSTRL0sHRxwdLWiBpeIPn78Yxxj8kHd5o2RpJxzjnxkq6RdJfosv/n6T7nXPjJD0k6dfbGXeNpCMlPdNo+Z3OuTGSDpJ0QjRoFEo6T1L98ovMLD+O2pE+OktqEiDMLOicm+ycW+9DTUBMzrm1zrkpjZebWdCPegAA6a/NBAgz+4eZnW1mATN71cxGN1pllqT6b/dHSLpL0iFmliupl3Nu2Y7mcM6tkRRptGytc25r9Gm1pFD08WLV7ShKUhdJxWaWa2azzGz36Ld688ysi3Ouwjm3McZ8X0b/jkTHDUuqlLRaUl70T6Wk2h3VjrRyqaSR0e7UfDN70MymSfppdFlfM+tuZm9Gn882syGSFF13qpm9HO1M9Ywuv9TMPjCzR6Nj9m84oZn1i75nRvTvpHQ50D6Z2Y1mNifaPZ1U38kysz832l4vMbP3o+ud02iMIjN7KrodzzCzwb78MACAlMvyu4CokWb21g7WuVTSDNV1E950zr3f6PV5kh4ws2xJTnUdh1skLZI0X5LM7EBJ18cY+2rn3IztTR7tAlwr6fzoojckvWpm50vKlTTKOVcdff5PSVskTXbObdrBzyUz+5mkb+pDjpm9Iulz1QW8a51zNTsaA2nlfyTt4Zw7wsz+LKmPc+54STKzSdF1tkg61jlXY2bHSrpcdZ0nSVrsnPuFmV2pup24pySdJekASR0lfRNjzpslXeOcm2tmJ0j6vaTfevTzoQ0zs4mS+kk6yDnnzGyQpFMbrFLtnDvezPaUdKekg51zoRgdiSskPeece8LMRki6QdIpqfgZAAD+SpcAscA5d0T9k1jH2Drnqszsn5JuktSnmdeLJZ0k6SPnXLGZ9VZdV2JWdJ05ksYlWlw0lDwp6Ubn3JLo4hsl/dE595yZnSHpOkkXO+c+N7NvJXV1zr0Xx9hHSPq5pOOiz4dIOlnSQNUFiLfN7AXn3KpE60baiLUddJZ0Z3QbzZG0tcFrC6J/L5c0SNIASYuccyFJpWb2WYzx9pJ0g9Wd9pMliePU0Zw9Jc10zrno83Cj1+u31z0kzYpud3LONV5vL0mHmdlF0echAUlmZr9SXTD9yjl3gd/1IPOwDcbWlg5h6qO6b/+vUd3OeiyzJP1O0uzo89Wq+2bt3egYB0YPGWn8Z/x25g1IekTSC865Fxq+JKkk+rhYUtfo+kdKypZUYmbH7+BnGh39eU5xzlU2GHerc646uqxaUsH2xkHaqdEPw3njHS9JOlN1QXespKtV9/+9nmvw2CQtkzTczLKi58gMjTHeYkn/7Zwb55w7RNKFragf7dsiSYc1eN74c6B+e10s6aD6zkP038KGFku6KbrNjZM00YNakeGcc3dEtzF23OALtsHY0qUDsV3RD65/qu6QoLlm9oSZTXTOvdJo1VmSpkiaG30+W9IJqvvA3GEHIpoyT5c0LHpM8CRJ+0r6kaReZnampE+dc79W3eFM95hZSHWBYVL0ePW/Sjpadd/GvWFmH0oqlfSc6r7RG25mrzjn/iTp/ujUL0S/OZ7inFsQPXdirup2Hmc65z5vwX82+GetpEoze1ZST8XuBrwm6TEzG6u6HbFmOefWmdljkt6X9IWklaoLKTkNVpuiuo5Gfdh8QHXBF/gB59wrZjbOzOao7hyrJ5tZb7GZvSjpPTMrV93FIh5qsMpfJd1tZr9W3b9VL6vusFEAQDtn33exAaQrM8t2ztWaWSdJH0kaEuOQEgAAAM+1iQ4EAF1uZhMkFUm6ivAAAAD8QgcCAAAAQNzazEnUAAAAAPxHgAAAAAAQt+2eA/HJyhDHN2WQvftm2Y7XSr28fX/FdphBKj+6I+22Q7bBzJKO26DEdphp2A6RDprbDulAAAAAAIgbAQIAAABA3AgQAAAAAOJGgAAAAAAQNwIEAAAAgLgRIHbAOafNZbV+lwFIRb38rgAAAIAAsT3OOa0uqdY9L65SyZYav8tBJtt5mE6fdILUrZ/flQAAgAxHgNiOcMTpvn+vVlVNRA9NX+N3OchUwSydf/545XfI1knnHu13NQAAIMMRIJrhnNPyddXbnocjTqvWV/lYETLWrntte5gVCEg7DfWxGAAAkOkIEM2IOOnhV7/vOlRWR/TMW8U+VoSMFAjq/DMP2va0MC9bx516sI8FAQCATEeAiME5p6XflTdZXhuK6OtVFT5UhEzVYY9RTZblZgVlg/bzoRoAAAACRLOejdFtKK+K6NV5G3yoBpnqZyc1DQpF+Tk66pi9fagGAACAANGEc04ffFba7OuV1REtWVaWwoqQqTqPOrzZ1/I7ZCl3+JgUVgMAAFCHANHIe4u26JW5zXcZyirDmvXJ5hRWhEzU45CjdNLRw5p9vUt+rg48eLcUVgQAAFCHANHIGx9s3OE6WyvC+vCL5rsUQGsdd/hgmdl21+lamKvC/camqCIAAIA6BIgGXpsf3/kNZZVhvfXRJi34nBCB5NvlqB/HtV6X/FwdMW6IOo08zOOKAAAAvkeAaGDu4i1xr7u1IhzzSk1Aa00YtcsOuw/1uhbkavfhO3lcEQAAwPcIEFHPv1Ms5xJ7z7qNNQmFDmBHhv3kJMWZHbbp2yNfXcdM8KYgAACARggQkp6euU6ffp34lZXKKsNavo67UyM5Rpx2qsYM7x1396Fel/xc7dK/q0dVAQAA/BABQtKXKyuUYPNhm+XrqjT7U67KhNbbc2A3BQIJth+iBu/cWT0OOSrJFQEAADSV8QHi4VfXKBRqaXyQyqvCKt5Uk8SKkIlGnX2GcoIt/3Usys9Rj56FSawIAAAgtowPEKtLqlvcfaj3xYoKvbtwU1LqQWYa2Luwxd2HensP7qaeY49OUkUAAACxZXSAeODlVaquibR6nKqaiGYv2sKhTGiRQy84U3k5Wa0eJ79DtiYc2F/dDz4yCVUBAADEltEBYmNpqNXdh3rVNRFVVIWTNBoySa+ivFZ3H+p1zM1Sx/zcpIwFAAAQS8YGiHunrUz6Dv/8paWas4guBOI3ftLZKszLTuqYh47sq64HcllXAADgjdYfN9EG3TttpdZsSP6Jz7Vhp5pWnJCNzDLuwrPUv2dBwpdt3ZEO2UFl5yQ3lAAAANTLyA5ETa13O/nvLNyk95dwcznsWF5OVtLDQ71jDhmgLqPHezI2AADIbBkXIKa+tEobSms9Gz8SkSIRuhDYvsN+cZb6dMnzbPysYECBVlwWFgAAoDkZt4eRip371+Zv1ILPSz2fB21XVtA86z7UO2HCEHUaeZincwAAgMyTMQHCOacHXl6ltRtTc9M35+rmBBo79IIztWuPAs/nMfM+pAAAgMyTMQHikdfWakVxdcrme3lOiT79powQgR8Yfc4ZGrJTUcp27E8+dg/ljzgkJXMBAIDMkBEBIhJx8mM//vl31uuLFRWpnxjpyYeOgJnp9BNGKHv3USmdFwAAtF8ZESCenLFO366p9GXucMQpQhcCkvY741QN36WLL3MHggGJw5kAAEAStPsAURuK+LoD//TMYn272p/wgjSSlaNAwL9ftzNPHikbuK9v8wMAgPaj3QeIl2aX6KuV/u7AV9dGFObSrhlt+AnHacTAbr7WkJObIwWCvtYAAADavnYdIKpqwqoNR/wuQ0/PLNaq9ak7gRtpJrejsrP9/1U785SRUt9hfpcBAADaOP/3ajxSURXWy++V6LPv0uMk5q0VIYXCdCEyTl6h9jxuokYO6el3JZKk3MICKZjldxkAAKANa7cB4t1PNmvRt+V+l7HNM28Va/3m1NyDAulj17HjNHqPXn6Xsc2ZJ4+UevT3uwwAANCGtcsAUVoeUnll2O8ymli/uUahNDikCilS2E0FBTl+V9FEx5696EIAAIAWa5cBYsHnpfr0mzK/y2ji+XfWa+myCg5lyhA7HTBaY4b39ruMJs44cR91HD6aEAEAAFqk3QWITVtrtbG01u8ymvXcO8Uqqwz5XQa81rm3unbt6HcVzTrjxH2kAn+vCgUAANqmdhcgliwrT6tzH2L5cmUlhzK1c92H75VW5z7EUjRkGF0IAACQsLQNEM45Lfv09YTeU7KlRms2pP/lUl+ZU6KqGgJEWzHq7DMSe0O3fuqzU5E3xSTRKccOlzoU+F0GAABoY9IyQDjn9PWCaZo37UZ9/eG/43pPyZYavfXRJi1O8+5DvY+/LONciDbg6IvP1fSLD9L4SWfH94Zu/bTn4aM1alh6dx/q9dh3FF0IAACQkLQMEJL04X9ul5zTR6/eEdf6q9ZXt5nwIElvLtio2hBdiHT3xLn7KxAwPX7u/nGtn99317Q/dKmh48fvJmV38LsMAADQhqRlgFj89oPbHrtIWEvefWS76xdvqtHSZW0nPNR795PNCtOFSFuT/nzxtsfBgOnsK3+5/Tf06K+hw3fyuKrk22XsOCkQ9LsMAADQRqRlgFgy6/vA4FxEn815Yrvrl2yp0ecr0uOO04mYs2iLXpu/QZEIISIdXT9x922PgwHTdROHbnf93B69te/gHl6XlXRHjtlVA4/5kWTmdykAAKANSLsA8cErf2uyLByq0Uev/SPm+sWbavTBZ1u9Lssz85aWivyQfm763ymyRjvUuVkB/eHmybHf0KO/9jlgQAoq88bh+/ejCwEAAOKSVgHi/Rdv0DcxTpp2kbC+i3FFppLNNZo+t0TfrqlMRXmeeeHdYkUcKSJd3H3v73TB6P5NlmcFA/rFqF2bvqFbPx3444O0565dvS/OQ3uddCJdCAAAsENpFSBWLHmr2ddqqys0b9qNP1hWXhXWsrVVHlflvcXflkvkh7Rx0l59m3Qf6uXnBnXnPZf9cGFBF+3Rr0sKKvNWW7lyFAAA8FfaBIh3n/yDIuHm79DsIiEtXzxT7794vSRpw5YavTZvQ6rK89wjr62Vowvhu+ce+f/KCjb/LXxWMKBT9+6ne6b+vm5Bt34ad/SIFFXnvTHnJHjPCwAAkHHSJkAUL/tIO/oaPhKuVcmKRZKk6lqn1RtqUlBZarT1w7DaiwMHdGu2+1AvOyugQ3btXvckt6MG9e6UgspSY/gubb+TAgAAvJUWAWLGQ5MVro0vDFSWlujfD/xBz75d7HFVqXfPi6voQvjorWeuVW52fL8SPQpz9eZT1+hHJx/kcVWpd8Qvz/G7BAAAkMZ8vwXtmw/+RhtWLVG8JwFEIiFtKv5OG63W28J8sG5T++motDXvPPtXDe/baYfdh3rZWQEN6Jmv3l06elxZ6u3SPd/vEgAAQBrzvQNRtmm1lOC37jmhYvXd8L8eVeSv259Z4XcJGWmnrnkKBBK7AlFRXrZ+c2CMqzK1A8ddcp7fJQAAgDTlawfi9fsuUnXF5oTfZwqrb1GlTjt9l1bXkFd4pGSlrR6nsdrKKxSqPbEF7+Qymqk258Xr1aVjdsLvCwRMVbVh3X/rox5UBaTWwuk3qaBD8j8S/uvB+Xr/oceTPi4AwD++Bojqyi0Jdx/qlRZ/rQ9fuEpjz7i+xfN3KDhCFljuyaXvXd6fVFPRUeHQxOQPjqQq6pidcPeh3qBeBZo29RId/19/SXJVQOosnH6T+nXLi/sQvkS8MGmMjqkKaeGTTyd9bACAP3w7hOnVqReqYkvLT4R2LqK138zXrKeuatH7OxQcJQt869l9s8yqlNNxigJZM72ZAEkx76Ub1Lsot8XvDwZMBw/qrqf+1bLtEPDbR6/c6Fl4kKQO2UG9PvlQDf5xSzqyAIB05EuAeOOBi7Vl3detH8g5RUKJnXicm/8T5XXaXRb42vOb7prVKLfjRcrrtLsCwQ+9nQwJm/38dRrcK7/VO06BgKljdjBJVQGpMfv561Q853b179H634Edyc4KaM4fx6t4zu3qeuAET+cCAHjPlwARCSfvCkprv/lA7z17dVzr5uafpkDwE5nVeh4e6pmFZFar3PzTZIElqZkUcckKWtJ2nA4e3F3/+ueVSRkL8Npbz1yrPfp2UnZW6j4CsoIBZWcF9OVtJ6pg30NTNi8AIPlSHiDefPA32pyM7kNDLhLH/ROcpEjKgkNjZpFoDdznIR28/exfNaRPYVLHDPq1cQEJCvi4rbb0fCMAQPpIaYBwrn4nOrlWfvau5k27cTtrRJTb8RwFs/w9jKhDwfGywDciRPjMzJNrXU0c3kd33/s7D0YGksRM/3niau21S5GvZay49zTlDBvtaw0AgJZLWYBwkbDefvT32rDSo8N4nJOLRGK8EFZOx0kKZs/2Zt4EmEl5hUfJAstFiPBJIKhXn/iLZztQATMpwPkQSEOBoJ7911UaPair35VIktb96yzZoP38LgMA0AIpCxCzn/6Tipd51wH4btEbWjD9NkXCoR8sz8n7rbKyZ3g2b0vkFY6XrMTvMjLSMw9dqVEDvduBOnWffrr59slSVo5ncwAtMfXeyzR+955+l/EDG584T+o1yO8yAAAJSkmACNdWKxIJez7PNx+9rE9n3t9gSbWkUHOr+8qsQnQhUqxDgbID3m/yF4weoCnXXOT5PEDcOhQoN+jbVbu3K1DY2e8SAAAJSsknyvyXb9Xar+elYiqFQzUK1VZLkrI7XK2snFdSMm+i8grHS0r+HbDRvKl3/Epjh/RIyVxFHYJSXnJP0gZa6uZbJum4PXfyu4yYNjz+c6lLetYGAIjN8wBRU1WmcHSHPhW++uAFfTH3qZTNhzaiqJcKUnivhl8fMkjnXHpmyuYDAABIFU8DRHVFqRZMv02rPp/l5TRN1FRuVW31SpmVpXTeRFlgpaRYJ34jqbrurAf+5+c6Zo8+KZ1256JcqahXSucEmui6s7rlZftdxXYV9B8k366xDQBImKcBYunsR7Vi8Uwvp4jpi3nP6st5JyocSs/Dl+rlFR4vqcrvMtq9y39/in6yd9+Uz3vZ4YM15fLTODwDvjr1vImasFt6B9kV954m5XXyuwwAQJw8CxCVW0tUVb7Jq+F3aNHbRVr2SUff5kea2GmoBnXL8236Px4xRON/eoRv8wNP3zJVT32y0u8yAADtiGcB4qsFL2n5oje9Gj4upSXZqixLzyuPIDUmXXiEThmR+u5DQyMHdJZ69Pe1BmS2978t1abyGr/LAAC0E57sXZdtWqPyTau8GDohi98u0uov/Pv2GT4buK9G9/P/SkhXThiiA44Z43cZyGDP3DpVb35V7HcZAIB2IukBomzTai1+50Et9+Hch1hKVuSqopQ7A2caG7Sf7rniKF/OfYhl4j59pD5D/C4DGezFT4pVsjV1V8QDALRfSQ8Qa7+er+8+fSPZw7bY0lmdVLIife8KHMyeJsn7m+xlmiOP3ks/3aef32VsM3nsIA0dNdzvMpDB/n37A1q4arPfZTTrsJ8dLwX4sgcA2oKkBojSDSu0fvmnyRwyKVZ9lqfyzen5wRTM+kTckTq5soYeoDP2T78rH/18wkCpLyEC/rn7veUq3pKeV347fp9eBAgAaCOSGiA2rlqqFUvS49Clhj6f20kfv95Z5VvS78OppvKPkrL8LqNdGTlmsE7ca2e/y2hi0oED9Lcrj5V2HuZ3KchQb9z1kC58cqGKS9PvUKYpl02VQpzoDQBtQdICxJb1y7Tys3eTNVzSfTmvUOWb2FFv73KHj9GUCYP9LqNZ5x7QX937p8d5GchMb099WOs2p2cXAgDQNiQlQJSWLNcnM6Zq9RfvJWM4zyydVZiWXQgkR/buo/T4FUfpyGHpfdOsm8/fX9ppqN9lIINNfnah1qdhFwIA0DYkJUCUb1mnNV/OTcZQnvp2YYGqyggQ7dXO/Xvp8KE9/S5jh07ca2cV9Er/OtF+ffjYU9wXAgDQYq0OEKUly/XZe48no5aU+HB6Zy7r2g5l7z5K9583yu8y4vb4ZROk3ul7qBXav9PvmqMNXNYVANACrQ4Q1RWbtf67hcmoJSVWfd5RtdXmdxmSpOqKv0nK9buMdqFLzy7ab0AXv8uI2yG7dVdWUdupF+3Pt9Onqbw6PS4hfeTfZ0nV5X6XAQCIU6sCxNYNK/Tx63clq5aUmfVkd1Vu9eQm3AkJ1x4miW5Ia2UNPUAvXHqY32UkbMZ1J0o9B/hdBjLYhGtf18Yy/w9l+uDld6RwyO8yAABxatVlifIKu2vfo3+VrFqSLjv3XgWzXo/5Wk5eJMXVwCuhlV/quJtm+F1Gy2xe63cFyACn/+5CXTxm15ivFXbg6nQAgMS06pMjKydP3dP4xlhmlysnr0zB7Lf8LqWJqrKHJRX4XUb7UL5ZG95Ln7ufA+nmicdm64wRfTR2SA+/S2liz8unS6XFfpcBAEiA/8fxeMi5PnKuk99lxBSJ7CYOXwKQEisXq7gyPU+YXrXoMw5fAoA2pl0HCEmqqbpK4dqD/S7jB6rKnpJcV7/LAJBBfnH5Y3r/641+l/EDu01+UVq/zO8yAAAJavcBQq6rqivuUDiUHpf4rCp7WpHwPqL7ACClSpbrmEl36uNlm/2uRJI0+JIXVDJnhhRJjytBAQDi1/4DhCSpk6rL71M4NMLXKqrKHlckPEKEBwC+2LJOh597i5asLPW1jKFTXtKGuTMJDwDQRmVIgJCkfPm94+6c/zUAyHDlmxWOOF9LqNhaQXgAgDYsgwKEVF3+mCLh3X2Zu6rsUbnIMF/mBoCGxp5+tb5cW+bL3MMue1llH8/yZW4AQHJk2AXAs1VVNk2S1KFwnAKB1Z7PWF1+l8KhCarLaulxB2wAGa62WqNO+INkpqX/uUG9O3fwfMqDrpuhpdOmccUlAGgHMqoDUScoKaiqre/IRbrLedTJd06qrrhV4dCR0TkJDwDSSCQshUMaduRvtcHju1EfdfssLX3+OcIDALQTGRgg6pkqt86VXGc5Z0kLEs5JzplqKq9VuPYEERwApLvBh1+qLRW1ckn+RsU5p5Pvm6f5Dz+R1HEBAP7KsEOYGjNVbl0gScrrtKecq/sWziyxk/ucM9VnsdrqSxSqvjipVQKA1/of9t+SpDWz/66sYN0XH1nBxL5jcs5tO0F78otL9Oj1dye3SABAWsjwAPG9ytJF0UdOeZ2Gx1jDSaqRlNvklXDt8aqpvMHD6gAgNfocfMm2x2vf+3uT181M2UFTTSjS5LUnFq7Q5F/e4ml9AAD/ESCaMFWWLomxeL065J+tqrLpqS8JAHzQ+6BLYiwcrA/uu0D7//jy1BcEAEgLGXwORIJcD8IDAKz9ivAAABmOAAEAAAAgbgQIAAAAAHEjQAAAAACIGwECAAAAQNwIEAAAAADiRoAAAAAAEDcCBAAAAIC4ESAAAAAAxI0AAQAAACBuBAgAAAAAcSNAAAAAAIgbAQIAAABA3AgQAAAAAOJGgAAAAAAQNwIEAAAAgLgRIAAAAADEjQABAAAAIG4ECAAAAABxI0AAAAAAiBsBAgAAAEDcCBAAAAAA4kaAAAAAABA3AgQAAACAuBEgAAAAAMSNAAEAAAAgbgQIAAAAAHEz55zfNQAAAABoI+hAAAAAAIgbAQIAAABA3AgQAAAAAOJGgAAAAAAQNwIEAAAAgLgRIAAAAADE7f8ADFAQjqypiOQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAJJUlEQVR4nO3df4xld1nH8c9TtzaNaCghLghEU0WgNdGNP0BAUpXGihESUCLxR1BIMEKjAWJETayCGkj/wKRgVOpq0ERNA4XEmobSgt3aUrI2QtU0NohGt2WhNIC4bN3y+Mc9Gyeb2d2nrXPvLPN6JZO958yZ8/3e2fPHfd9zzp3q7gAAAEyct+kJAAAA5w4BAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwNjGA6Kqvqmqbjpl3b2PYj83VNWB5fGLqurBqqpl+W1V9dODfby5qv5t63yq6kBV3VZVf1tVN1fVxcv6i5d1H6qqW6rqqWfY7zdX1eGq+q+qev6W9W+vqjuWr1/Zsv5NVfXRqrqzql7/SH8XAACwUzYeEP+PDiV53vL4eUkOJ7l0y/Ktg328M8n3n7LuviRXdPcLklyd5DeX9b+Q5NruvizJnya58gz7vS/J5UmuO2X9O7r7OUmem+QlS2h8bZKfS3Jy/c9X1dcM5s4eVFVftek5AAB7yzkTEFX1zqr6mao6r6purKpnn7LJoSQn393/9iS/n+T5VXVBkv3d/cmzjdHd9yX58inr7u/uLyyLx5OcWB7/Y5LHL48vSnK0qi6oqkNV9cyqetJyBuGi7v7v7v7sNuP9y/Lvl5f9PpzkWJIjSS5cvo4l+Z+zzZ3dqaourarbl7NUf1NVlyzHxV9X1V9V1VXLdvdu+Zl3VdVly+Mbl7Ncd1bV9y7rrqqqP6mq9yd5eVVdWVW3LuO8egNPEwDYQ/ZtegKL76yqD51lm9cnuTmrswkf7O6PnPL9O5P8cVWdn6SzOuNwdZK7k3w0SZYXYL+7zb5/q7tvPtPgy1mAtyR51bLqpiQ3VtWrklyQ5Hu6+/iyfDDJ55L8Unc/eJbnlar6ySSfOBk5VXVDknuyCry3dPdDZ9sHu9YPJTnY3X9YVecleW+SX+zu26vqjwY//9Lu/mJVPSvJO5L8wLL+eHe/eFl/dZIXZHW83FpV7+3uB3bguQAA7JqAONzdLzy5sN09EN39pao6mORtSZ58mu8fTfLSJHd199GqelJWZyUOLdvcnuSyRzq5JUr+Mslbu/ufltVvTfLr3f2eqnpFkt9J8truvqeq/jXJE7r77wb7fmGSn03yo8vytyZ5WZKLs3pB+OGqur67//ORzptd4WCSX6uqP0/ysSRPzyp2k+QjSba7d+bkvTsXJvm9qnpGVmennrJlm5PH1rcluSTJLcvy1yV5WhIBwWNSVa9L8mNJ7u1uZ7bYCMchm+YY3N5uCYizqqonZ/Xu/5uzerG+3c3Fh5L8cpJfXZaPJPnxrF6gP6ozEMu7xn+W5Pruvn7rt5J8Znl8NMkTlu0vT3J+ks9U1Yu7+/1neE7PXp7PD3f3sS37/UJ3H1+2OZ7kcafbB7ve8e5+Y5IsN+d/Ksl3ZRUP353V/TFJ8rkleD+d5DuSvDvJFUke7u7vq6pLkmw9lh5e/v3nJHcleVl3d1Wd390ueeMx6+5rklyz6XmwtzkO2TTH4PbOiYBYXsQfzOqSoDuq6i+q6kXdfcMpmx5K8oYkdyzLtyV5SVaXMZ31DMRSmT+R5FnLi73XJDmQ5EeS7K+qn0ry8e6+MqvLmf6gqk5kFQyvqaqvT/LbWV22ciLJTVX190k+n+Q9Wb1TfGlV3dDdv5Hk2mXo65cPjHpDdx9erne/I6uYuKW773kUvzZ2h1dU1Suzuqzu/qyOm3dV1QP5vwBNVmfWPpDVvTVHl3W3J3nTcizett3Ou/vu5fsfrqqHkxxbwvXEdtsDADxW1d2bngPsSUuQfkt3X7XpuQAATJ0zn8IEAABsnjMQAADAmDMQAADAmIAAAADGzvgpTJ964wnXN+0h+6/eV5uew3YuPPA6x+Eecuyua3bdcegY3Ft24zGYOA73Gschu8HpjkNnIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMLZvXQP9w3OP5NPf8MV1DbcR33jPRXn6x5+46WlwBle89pW5/JKv7P+jg7d8Mndfd92mpwEAfIVaW0D8+zMezCcu/ey6htuIC47tExC73Kuf87T84DP3b3oaO+rI5x/K3foBANghLmECAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjawuIXtdAAADAjllbQNS6BgIAAHaMS5gAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAsbUFRK9rIAAAYMesLSBqXQMBAAA7xiVMAADAmIAAAADGBAQAADDmJmoAAGDMTdQAAMCYS5gAAIAxAQEAAIwJCAAAYExAAAAAYz6FCQAAGPMpTAAAwJhLmAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxvwhOQAAYMwfkgMAAMZcwgQAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMrS0gel0DAQAAO2ZtAVHrGggAANgxLmECAADGBAQAADAmIAAAgDE3UQMAAGNuogYAAMb2rWugp977+Hz1sbUNtxH7/+Nxm54CZ/Huw0fywJceGm/f3amq0y7vpEc79s0fu38npwUA7HFre0V/4NanrGsoOK33vf3avG/TkwAAOIe5iRoAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABj1d2bngMAAHCOcAYCAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACM/S9c3eagrERZlAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1SBhKk9Bps6",
        "colab_type": "text"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-3_dG9aBps7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "ea6a5bbc-c196-46ff-e934-7cff02b9ce5f"
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1822: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Mask_RCNN/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Mask_RCNN/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Mask_RCNN/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "02GQgEQKBps-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"coco\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "    # See README for instructions to download the COCO weights\n",
        "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZV5KRQlBptD",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train in two stages:\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ZvE645hVBptE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1270d3eb-690d-4ad2-9482-141d2f5e25ce"
      },
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE, \n",
        "            epochs=1, \n",
        "            layers='heads')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/drive/My Drive/Mask_RCNN/logs/shapes20200606T0849/mask_rcnn_shapes_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:705: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:708: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:2039: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-1:\n",
            "Process Process-2:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 631, in data_generator_task\n",
            "    self.queue.put(generator_output)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 631, in data_generator_task\n",
            "    self.queue.put(generator_output)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1056, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eP9rv-joBptI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=2, \n",
        "            layers=\"all\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1ezFZSBptN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights\n",
        "# Typically not needed because callbacks save after every epoch\n",
        "# Uncomment to save manually\n",
        "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
        "# model.keras_model.save_weights(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFHU9C7NBptS",
        "colab_type": "text"
      },
      "source": [
        "## Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKK2449BptT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceConfig(ShapesConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLg-vhitBptW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uodk-CJ-Bptb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlqWj-AjBpte",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDnQMXcYBptf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG2qA0DKBpti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}